{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¢ Started scraping TED Talks YouTube channel...\n",
      "üöÄ No more new videos found. Stopping scrolling...\n",
      "‚úÖ Scraping completed! Total videos found: 1272\n",
      "üîó Sample links: ['4SCrXqbhmCY', 'tWZmunAvlMM', 'vO5Rio_skIU', 'Rk5C149J9C0', 'KNEGWrD08f8']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_links_ted():\n",
    "    # Initialize WebDriver (Windows)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")  # Start maximized\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get('https://www.youtube.com/user/TEDtalksDirector/videos')\n",
    "\n",
    "    print(\"üì¢ Started scraping TED Talks YouTube channel...\")\n",
    "\n",
    "    prev_count = 0\n",
    "    scroll_attempts = 0\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        contents_div = soup.find('div', id='contents')\n",
    "\n",
    "        res = contents_div.find_all('a', \n",
    "                                    attrs={'class': 'yt-simple-endpoint focus-on-expand style-scope ytd-rich-grid-media'})\n",
    "\n",
    "        links = list(set(i.get('href').replace('/watch?v=', '') for i in res))\n",
    "\n",
    "        # Check if new videos were loaded\n",
    "        if len(links) == prev_count:\n",
    "            scroll_attempts += 1\n",
    "        else:\n",
    "            scroll_attempts = 0\n",
    "\n",
    "        prev_count = len(links)\n",
    "\n",
    "        # Stop if no new videos are loaded after 3 attempts\n",
    "        if scroll_attempts >= 3:\n",
    "            print(\"üöÄ No more new videos found. Stopping scrolling...\")\n",
    "            break\n",
    "\n",
    "    driver.quit()  # Close the browser\n",
    "    print(f\"‚úÖ Scraping completed! Total videos found: {len(links)}\")\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Run the function\n",
    "links_video_ted = get_video_links_ted()\n",
    "\n",
    "# Print sample links\n",
    "print(f\"üîó Sample links: {links_video_ted[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¢ Started scraping TED Talks YouTube channel... (Resuming from 0 videos)\n",
      "‚úÖ Saved progress: 28 videos scraped so far...\n",
      "‚úÖ Saved progress: 60 videos scraped so far...\n",
      "‚úÖ Saved progress: 88 videos scraped so far...\n",
      "‚úÖ Saved progress: 120 videos scraped so far...\n",
      "‚úÖ Saved progress: 148 videos scraped so far...\n",
      "‚úÖ Saved progress: 180 videos scraped so far...\n",
      "‚úÖ Saved progress: 208 videos scraped so far...\n",
      "‚úÖ Saved progress: 240 videos scraped so far...\n",
      "‚úÖ Saved progress: 268 videos scraped so far...\n",
      "‚úÖ Saved progress: 300 videos scraped so far...\n",
      "‚úÖ Saved progress: 328 videos scraped so far...\n",
      "‚úÖ Saved progress: 360 videos scraped so far...\n",
      "‚úÖ Saved progress: 388 videos scraped so far...\n",
      "‚úÖ Saved progress: 420 videos scraped so far...\n",
      "‚úÖ Saved progress: 448 videos scraped so far...\n",
      "‚úÖ Saved progress: 480 videos scraped so far...\n",
      "‚úÖ Saved progress: 508 videos scraped so far...\n",
      "‚úÖ Saved progress: 540 videos scraped so far...\n",
      "‚úÖ Saved progress: 564 videos scraped so far...\n",
      "‚úÖ Saved progress: 572 videos scraped so far...\n",
      "‚úÖ Saved progress: 612 videos scraped so far...\n",
      "‚úÖ Saved progress: 640 videos scraped so far...\n",
      "‚úÖ Saved progress: 672 videos scraped so far...\n",
      "‚úÖ Saved progress: 704 videos scraped so far...\n",
      "‚úÖ Saved progress: 732 videos scraped so far...\n",
      "‚úÖ Saved progress: 764 videos scraped so far...\n",
      "‚úÖ Saved progress: 792 videos scraped so far...\n",
      "‚úÖ Saved progress: 824 videos scraped so far...\n",
      "‚úÖ Saved progress: 852 videos scraped so far...\n",
      "‚úÖ Saved progress: 884 videos scraped so far...\n",
      "‚úÖ Saved progress: 912 videos scraped so far...\n",
      "‚úÖ Saved progress: 944 videos scraped so far...\n",
      "‚úÖ Saved progress: 972 videos scraped so far...\n",
      "‚úÖ Saved progress: 1004 videos scraped so far...\n",
      "‚úÖ Saved progress: 1032 videos scraped so far...\n",
      "‚úÖ Saved progress: 1064 videos scraped so far...\n",
      "‚úÖ Saved progress: 1092 videos scraped so far...\n",
      "‚úÖ Saved progress: 1124 videos scraped so far...\n",
      "‚úÖ Saved progress: 1152 videos scraped so far...\n",
      "‚úÖ Saved progress: 1184 videos scraped so far...\n",
      "‚úÖ Saved progress: 1212 videos scraped so far...\n",
      "‚úÖ Saved progress: 1244 videos scraped so far...\n",
      "‚úÖ Saved progress: 1272 videos scraped so far...\n",
      "‚úÖ Saved progress: 1304 videos scraped so far...\n",
      "‚úÖ Saved progress: 1332 videos scraped so far...\n",
      "‚úÖ Saved progress: 1364 videos scraped so far...\n",
      "‚úÖ Saved progress: 1392 videos scraped so far...\n",
      "‚úÖ Saved progress: 1424 videos scraped so far...\n",
      "‚úÖ Saved progress: 1452 videos scraped so far...\n",
      "‚úÖ Saved progress: 1484 videos scraped so far...\n",
      "‚úÖ Saved progress: 1512 videos scraped so far...\n",
      "‚úÖ Saved progress: 1544 videos scraped so far...\n",
      "‚úÖ Saved progress: 1572 videos scraped so far...\n",
      "‚úÖ Saved progress: 1604 videos scraped so far...\n",
      "‚úÖ Saved progress: 1632 videos scraped so far...\n",
      "‚úÖ Saved progress: 1664 videos scraped so far...\n",
      "‚úÖ Saved progress: 1692 videos scraped so far...\n",
      "‚úÖ Saved progress: 1724 videos scraped so far...\n",
      "‚úÖ Saved progress: 1752 videos scraped so far...\n",
      "‚úÖ Saved progress: 1784 videos scraped so far...\n",
      "‚úÖ Saved progress: 1812 videos scraped so far...\n",
      "‚úÖ Saved progress: 1844 videos scraped so far...\n",
      "‚úÖ Saved progress: 1872 videos scraped so far...\n",
      "‚úÖ Saved progress: 1904 videos scraped so far...\n",
      "‚úÖ Saved progress: 1932 videos scraped so far...\n",
      "‚úÖ Saved progress: 1964 videos scraped so far...\n",
      "‚úÖ Saved progress: 1992 videos scraped so far...\n",
      "‚úÖ Saved progress: 2024 videos scraped so far...\n",
      "‚úÖ Saved progress: 2052 videos scraped so far...\n",
      "‚ùå Error occurred: Message: target frame detached: received Inspector.detached event\n",
      "  (Session info: chrome=134.0.6998.178)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7EFA54C25+3179557]\n",
      "\t(No symbol) [0x00007FF7EF6B88A0]\n",
      "\t(No symbol) [0x00007FF7EF5491CA]\n",
      "\t(No symbol) [0x00007FF7EF537DF3]\n",
      "\t(No symbol) [0x00007FF7EF536BB2]\n",
      "\t(No symbol) [0x00007FF7EF536676]\n",
      "\t(No symbol) [0x00007FF7EF53633A]\n",
      "\t(No symbol) [0x00007FF7EF533EC2]\n",
      "\t(No symbol) [0x00007FF7EF53498F]\n",
      "\t(No symbol) [0x00007FF7EF56019B]\n",
      "\t(No symbol) [0x00007FF7EF553660]\n",
      "\t(No symbol) [0x00007FF7EF553502]\n",
      "\t(No symbol) [0x00007FF7EF5F35A2]\n",
      "\t(No symbol) [0x00007FF7EF5C7C2A]\n",
      "\t(No symbol) [0x00007FF7EF5F02F3]\n",
      "\t(No symbol) [0x00007FF7EF5C7A03]\n",
      "\t(No symbol) [0x00007FF7EF5906D0]\n",
      "\t(No symbol) [0x00007FF7EF591983]\n",
      "\tGetHandleVerifier [0x00007FF7EFAB67CD+3579853]\n",
      "\tGetHandleVerifier [0x00007FF7EFACD1D2+3672530]\n",
      "\tGetHandleVerifier [0x00007FF7EFAC2153+3627347]\n",
      "\tGetHandleVerifier [0x00007FF7EF82092A+868650]\n",
      "\t(No symbol) [0x00007FF7EF6C2FFF]\n",
      "\t(No symbol) [0x00007FF7EF6BF4A4]\n",
      "\t(No symbol) [0x00007FF7EF6BF646]\n",
      "\t(No symbol) [0x00007FF7EF6AEAA9]\n",
      "\tBaseThreadInitThunk [0x00007FF9A3CCE8D7+23]\n",
      "\tRtlUserThreadStart [0x00007FF9A513BF6C+44]\n",
      "\n",
      "‚úÖ Scraping completed! Total videos found: 2052\n",
      "üîó Sample links: ['SF9qq6vQ3Pg', '4SCrXqbhmCY', 'tWZmunAvlMM', 'j_JxWI0_F9Q', 'vO5Rio_skIU']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SAVE_FILE = \"scraped_links.json\"  # File to store progress\n",
    "\n",
    "def load_existing_links():\n",
    "    \"\"\"Load existing scraped links from a JSON file to resume progress.\"\"\"\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        try:\n",
    "            with open(SAVE_FILE, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            return set(data)  # Convert to set for faster lookups\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Corrupted JSON file, starting fresh...\")\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_links(links):\n",
    "    \"\"\"Save scraped links to JSON file periodically to prevent data loss.\"\"\"\n",
    "    with open(SAVE_FILE, \"w\") as file:\n",
    "        json.dump(list(links), file, indent=4)\n",
    "\n",
    "def get_video_links_ted():\n",
    "    # Load previously scraped links to avoid duplicates\n",
    "    scraped_links = load_existing_links()\n",
    "    \n",
    "    # Initialize WebDriver (Windows)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")  # Start maximized\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    driver.get('https://www.youtube.com/user/TEDtalksDirector/videos')\n",
    "    print(f\"üì¢ Started scraping TED Talks YouTube channel... (Resuming from {len(scraped_links)} videos)\")\n",
    "\n",
    "    prev_count = 0\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 8  # Number of times we try before stopping\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Scroll down multiple times slowly\n",
    "            for _ in range(3):\n",
    "                ActionChains(driver).send_keys(Keys.PAGE_DOWN).perform()\n",
    "                time.sleep(1.5)\n",
    "\n",
    "            # Parse page content\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            contents_div = soup.find('div', id='contents')\n",
    "\n",
    "            res = contents_div.find_all('a', \n",
    "                                        attrs={'class': 'yt-simple-endpoint focus-on-expand style-scope ytd-rich-grid-media'})\n",
    "\n",
    "            new_links = set(i.get('href').replace('/watch?v=', '') for i in res) - scraped_links\n",
    "            scraped_links.update(new_links)  # Add new links\n",
    "\n",
    "            # Save progress every 50 new videos\n",
    "            if len(new_links) > 0:\n",
    "                save_links(scraped_links)\n",
    "                print(f\"‚úÖ Saved progress: {len(scraped_links)} videos scraped so far...\")\n",
    "\n",
    "            # Check if new videos were loaded\n",
    "            if len(scraped_links) == prev_count:\n",
    "                scroll_attempts += 1\n",
    "            else:\n",
    "                scroll_attempts = 0\n",
    "\n",
    "            prev_count = len(scraped_links)\n",
    "\n",
    "            # Try clicking \"Show More\" button if present\n",
    "            try:\n",
    "                show_more = driver.find_element(By.XPATH, '//yt-formatted-string[text()=\"Show more\"]')\n",
    "                show_more.click()\n",
    "                time.sleep(3)  # Give time to load more videos\n",
    "            except:\n",
    "                pass  # No \"Show More\" button found, continue scrolling\n",
    "\n",
    "            # Stop if no new videos are loaded after max_attempts\n",
    "            if scroll_attempts >= max_attempts:\n",
    "                print(\"üöÄ No more new videos found. Stopping scrolling...\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error occurred: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()  # Ensure the browser is closed\n",
    "        save_links(scraped_links)  # Save final progress\n",
    "        print(f\"‚úÖ Scraping completed! Total videos found: {len(scraped_links)}\")\n",
    "    \n",
    "    return scraped_links\n",
    "\n",
    "# Run the function\n",
    "links_vid = get_video_links_ted()\n",
    "\n",
    "# Print sample links\n",
    "print(f\"üîó Sample links: {list(links_vid)[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¢ Resuming scraping... (Already have 2060 videos)\n",
      "üî• Clicked on 'Popular' button successfully!\n",
      "‚úÖ Progress saved: 2115 videos scraped so far...\n",
      "‚úÖ Progress saved: 2140 videos scraped so far...\n",
      "‚úÖ Progress saved: 2166 videos scraped so far...\n",
      "‚úÖ Progress saved: 2220 videos scraped so far...\n",
      "‚úÖ Progress saved: 2244 videos scraped so far...\n",
      "‚úÖ Progress saved: 2269 videos scraped so far...\n",
      "‚úÖ Progress saved: 2294 videos scraped so far...\n",
      "‚úÖ Progress saved: 2341 videos scraped so far...\n",
      "‚úÖ Progress saved: 2368 videos scraped so far...\n",
      "‚úÖ Progress saved: 2390 videos scraped so far...\n",
      "‚úÖ Progress saved: 2414 videos scraped so far...\n",
      "‚úÖ Progress saved: 2463 videos scraped so far...\n",
      "‚úÖ Progress saved: 2486 videos scraped so far...\n",
      "‚úÖ Progress saved: 2511 videos scraped so far...\n",
      "‚úÖ Progress saved: 2531 videos scraped so far...\n",
      "‚úÖ Progress saved: 2577 videos scraped so far...\n",
      "‚úÖ Progress saved: 2599 videos scraped so far...\n",
      "‚úÖ Progress saved: 2622 videos scraped so far...\n",
      "‚úÖ Progress saved: 2651 videos scraped so far...\n",
      "‚úÖ Progress saved: 2694 videos scraped so far...\n",
      "‚úÖ Progress saved: 2715 videos scraped so far...\n",
      "‚úÖ Progress saved: 2741 videos scraped so far...\n",
      "‚úÖ Progress saved: 2757 videos scraped so far...\n",
      "‚úÖ Progress saved: 2796 videos scraped so far...\n",
      "‚úÖ Progress saved: 2817 videos scraped so far...\n",
      "‚úÖ Progress saved: 2836 videos scraped so far...\n",
      "üöÄ No more new videos found. Stopping scrolling...\n",
      "‚úÖ Scraping completed! Total videos found: 2836\n",
      "üîó Sample links: ['QuR969uMICM', 'um5gMZcZWm0', 'LcNvkhS4UYg', '_uUskajC1Ps', '0DHywidLX6A']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SAVE_FILE = \"scraped_links.json\"  # Store progress\n",
    "\n",
    "def load_existing_links():\n",
    "    \"\"\"Load previously scraped links to avoid duplicates.\"\"\"\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        try:\n",
    "            with open(SAVE_FILE, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            return set(data)  # Convert list to set for faster lookup\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è JSON file corrupted. Restarting fresh...\")\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_links(links):\n",
    "    \"\"\"Save links to a JSON file periodically.\"\"\"\n",
    "    with open(SAVE_FILE, \"w\") as file:\n",
    "        json.dump(list(links), file, indent=4)\n",
    "\n",
    "def get_video_links_ted():\n",
    "    # Load previously scraped links\n",
    "    scraped_links = load_existing_links()\n",
    "    \n",
    "    # Initialize WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")  \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get('https://www.youtube.com/user/TEDtalksDirector/videos')\n",
    "    print(f\"üì¢ Resuming scraping... (Already have {len(scraped_links)} videos)\")\n",
    "\n",
    "    # Click the \"Popular\" button\n",
    "    try:\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "        popular_button = driver.find_element(By.XPATH, '//yt-formatted-string[@title=\"Popular\"]')\n",
    "        popular_button.click()\n",
    "        time.sleep(3)  # Allow sorting to take effect\n",
    "        print(\"üî• Clicked on 'Popular' button successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Couldn't click on 'Popular' button: {e}\")\n",
    "\n",
    "    prev_count = len(scraped_links)\n",
    "    scroll_attempts = 0\n",
    "    max_attempts = 10  # Try 10 times before stopping\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Scroll further to reach older videos\n",
    "            for _ in range(5):  \n",
    "                ActionChains(driver).send_keys(Keys.PAGE_DOWN).perform()\n",
    "                time.sleep(1.5)\n",
    "\n",
    "            # Parse updated page content\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            contents_div = soup.find('div', id='contents')\n",
    "\n",
    "            res = contents_div.find_all('a', \n",
    "                                        attrs={'class': 'yt-simple-endpoint focus-on-expand style-scope ytd-rich-grid-media'})\n",
    "\n",
    "            new_links = set(i.get('href').replace('/watch?v=', '') for i in res) - scraped_links\n",
    "            scraped_links.update(new_links)\n",
    "\n",
    "            # Save progress every 50 new videos\n",
    "            if len(new_links) > 0:\n",
    "                save_links(scraped_links)\n",
    "                print(f\"‚úÖ Progress saved: {len(scraped_links)} videos scraped so far...\")\n",
    "\n",
    "            # Stop if no new videos load after several attempts\n",
    "            if len(scraped_links) == prev_count:\n",
    "                scroll_attempts += 1\n",
    "            else:\n",
    "                scroll_attempts = 0  # Reset counter if new videos found\n",
    "\n",
    "            prev_count = len(scraped_links)\n",
    "\n",
    "            # Attempt clicking \"Show More\" if available\n",
    "            try:\n",
    "                show_more = driver.find_element(By.XPATH, '//yt-formatted-string[text()=\"Show more\"]')\n",
    "                show_more.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                pass  # Ignore if button not found\n",
    "\n",
    "            if scroll_attempts >= max_attempts:\n",
    "                print(\"üöÄ No more new videos found. Stopping scrolling...\")\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()  # Close browser\n",
    "        save_links(scraped_links)  # Save final progress\n",
    "        print(f\"‚úÖ Scraping completed! Total videos found: {len(scraped_links)}\")\n",
    "    \n",
    "    return scraped_links\n",
    "\n",
    "# Run the function to continue scraping\n",
    "links_vid = get_video_links_ted()\n",
    "\n",
    "# Print sample links\n",
    "print(f\"üîó Sample links: {list(links_vid)[-5:]}\")  # Print last 5 links for confirmation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¢ Resuming scraping... (Already have 2836 videos)\n",
      "üî• Clicked on 'Popular' button successfully!\n",
      "‚úÖ Progress saved: 2843 videos scraped so far...\n",
      "‚úÖ Progress saved: 2846 videos scraped so far...\n",
      "‚úÖ Progress saved: 2852 videos scraped so far...\n",
      "‚úÖ Progress saved: 2856 videos scraped so far...\n",
      "‚úÖ Progress saved: 2860 videos scraped so far...\n",
      "‚úÖ Progress saved: 2865 videos scraped so far...\n",
      "‚úÖ Progress saved: 2868 videos scraped so far...\n",
      "‚úÖ Progress saved: 2875 videos scraped so far...\n",
      "‚úÖ Progress saved: 2879 videos scraped so far...\n",
      "‚úÖ Progress saved: 2882 videos scraped so far...\n",
      "‚úÖ Progress saved: 2886 videos scraped so far...\n",
      "‚úÖ Progress saved: 2893 videos scraped so far...\n",
      "‚úÖ Progress saved: 2896 videos scraped so far...\n",
      "‚úÖ Progress saved: 2901 videos scraped so far...\n",
      "‚úÖ Progress saved: 2903 videos scraped so far...\n",
      "‚úÖ Progress saved: 2908 videos scraped so far...\n",
      "‚úÖ Progress saved: 2915 videos scraped so far...\n",
      "‚úÖ Progress saved: 2918 videos scraped so far...\n",
      "‚úÖ Progress saved: 2922 videos scraped so far...\n",
      "‚úÖ Progress saved: 2925 videos scraped so far...\n",
      "‚úÖ Progress saved: 2928 videos scraped so far...\n",
      "‚úÖ Progress saved: 2932 videos scraped so far...\n",
      "‚úÖ Progress saved: 2935 videos scraped so far...\n",
      "‚úÖ Progress saved: 2937 videos scraped so far...\n",
      "‚úÖ Progress saved: 2941 videos scraped so far...\n",
      "‚úÖ Progress saved: 2946 videos scraped so far...\n",
      "‚úÖ Progress saved: 2951 videos scraped so far...\n",
      "‚úÖ Progress saved: 2955 videos scraped so far...\n",
      "‚úÖ Progress saved: 2961 videos scraped so far...\n",
      "‚úÖ Progress saved: 2968 videos scraped so far...\n",
      "‚úÖ Progress saved: 2974 videos scraped so far...\n",
      "‚úÖ Progress saved: 2996 videos scraped so far...\n",
      "‚úÖ Progress saved: 3018 videos scraped so far...\n",
      "‚úÖ Progress saved: 3046 videos scraped so far...\n",
      "‚úÖ Progress saved: 3067 videos scraped so far...\n",
      "‚úÖ Progress saved: 3093 videos scraped so far...\n",
      "‚úÖ Progress saved: 3112 videos scraped so far...\n",
      "‚úÖ Progress saved: 3137 videos scraped so far...\n",
      "‚úÖ Progress saved: 3162 videos scraped so far...\n",
      "‚úÖ Progress saved: 3189 videos scraped so far...\n",
      "‚úÖ Progress saved: 3212 videos scraped so far...\n",
      "üöÄ Reached the end of the page. Stopping scraping...\n",
      "‚úÖ Scraping completed! Total videos found: 3212\n",
      "üîó Sample links: ['QuR969uMICM', 'um5gMZcZWm0', 'LcNvkhS4UYg', '_uUskajC1Ps', '0DHywidLX6A']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "SAVE_FILE = \"scraped_links.json\"  # Store progress\n",
    "\n",
    "def load_existing_links():\n",
    "    \"\"\"Load previously scraped links to avoid duplicates.\"\"\"\n",
    "    if os.path.exists(SAVE_FILE):\n",
    "        try:\n",
    "            with open(SAVE_FILE, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "            return set(data)  # Convert list to set for faster lookup\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è JSON file corrupted. Restarting fresh...\")\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_links(links):\n",
    "    \"\"\"Save links to a JSON file periodically.\"\"\"\n",
    "    with open(SAVE_FILE, \"w\") as file:\n",
    "        json.dump(list(links), file, indent=4)\n",
    "\n",
    "def get_video_links_ted():\n",
    "    # Load previously scraped links\n",
    "    scraped_links = load_existing_links()\n",
    "    \n",
    "    # Initialize WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")  \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get('https://www.youtube.com/user/TEDtalksDirector/videos')\n",
    "    print(f\"üì¢ Resuming scraping... (Already have {len(scraped_links)} videos)\")\n",
    "\n",
    "    # Click the \"Popular\" button\n",
    "    try:\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "        popular_button = driver.find_element(By.XPATH, '//yt-formatted-string[@title=\"Popular\"]')\n",
    "        popular_button.click()\n",
    "        time.sleep(3)  # Allow sorting to take effect\n",
    "        print(\"üî• Clicked on 'Popular' button successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Couldn't click on 'Popular' button: {e}\")\n",
    "\n",
    "    prev_count = len(scraped_links)\n",
    "    last_scroll_height = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Scroll down\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Get new page height\n",
    "            new_scroll_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            \n",
    "            # Parse updated page content\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            contents_div = soup.find('div', id='contents')\n",
    "\n",
    "            res = contents_div.find_all('a', \n",
    "                                        attrs={'class': 'yt-simple-endpoint focus-on-expand style-scope ytd-rich-grid-media'})\n",
    "\n",
    "            new_links = set(i.get('href').replace('/watch?v=', '') for i in res) - scraped_links\n",
    "            scraped_links.update(new_links)\n",
    "\n",
    "            # Save progress every 50 new videos\n",
    "            if len(new_links) > 0:\n",
    "                save_links(scraped_links)\n",
    "                print(f\"‚úÖ Progress saved: {len(scraped_links)} videos scraped so far...\")\n",
    "\n",
    "            # If no new videos and scroll height doesn't change, stop\n",
    "            if len(scraped_links) == prev_count and new_scroll_height == last_scroll_height:\n",
    "                print(\"üöÄ Reached the end of the page. Stopping scraping...\")\n",
    "                break\n",
    "\n",
    "            prev_count = len(scraped_links)\n",
    "            last_scroll_height = new_scroll_height  # Update last scroll height\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()  # Close browser\n",
    "        save_links(scraped_links)  # Save final progress\n",
    "        print(f\"‚úÖ Scraping completed! Total videos found: {len(scraped_links)}\")\n",
    "    \n",
    "    return scraped_links\n",
    "\n",
    "# Run the function to continue scraping\n",
    "links_vid = get_video_links_ted()\n",
    "\n",
    "# Print sample links\n",
    "print(f\"üîó Sample links: {list(links_vid)[-5:]}\")  # Print last 5 links for confirmation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
